---
title: "머신러닝 스터디 내용 정리"
date: 2018-07-17T17:39:55+09:00
categories: [dev]
tags: [python, tensorflow]
---

[강의자료](https://developers.google.com/machine-learning/crash-course/prereqs-and-prework)

# 1. 1주차

## 1.1. ML 소개

## 1.2. ML 문제로 표현하기

### 1.2.1. 주요 ML 용어

#### 1.2.1.1. 라벨

- 예측하는 항목
- [단순 선형 회귀](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80#%EB%8B%A8%EC%88%9C_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80%EC%99%80_%EB%8B%A4%EC%A4%91_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)의 y변수
- 예를 들어 사용자 클릭수는 좋은 학습 라벨로 사용할 수 있는 관찰 가능하고 수량화 가능한 측정항목

### 1.2.2. 특성

- 입력 변수
- [단순 선형 회귀](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80#%EB%8B%A8%EC%88%9C_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80%EC%99%80_%EB%8B%A4%EC%A4%91_%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80)의 x변수
- 구체적이고 수량화 가능한 특성이 좋은 특성

### 1.2.3. 예

- 데이터(**x**)의 특정 인스턴스 (x는 벡터)

- 두 카테고리로 구분
  - 라벨이 있는 예
    - 특성과 라벨이 모두 포함됨
    - labeled examples: {features, label}: (x, y)
  - 라벨이 없는 예
    - 특성은 포함, 라벨은 포함 X
    - unlabeled examples: {features, ?}: (x, ?)
- 라벨이 있는 예로 모델을 학습 시키고 해당 모델을 이용하여 라벨이 없는 예를 예측

### 1.2.4. 모델

- 특성과 라벨의 관계 정의
- 모델 수명의 두 단계
  - 학습 : 모델을 만들거나 배우는 것을 의미. 
  - 추론 : 학습된 모델을 라벨이 없는 예에 적용하는것을 의미. 즉 학습된 모델을 사용하여 유용한 예측(y')을 해냄.

### 1.2.5. 회귀와 분류

- 회귀 : 연속적인 값을 예측
- 분류 : 불연속적인 값을 예측



## 1.3. ML로 전환하기

### 1.3.1. 선형 회귀

- 점 집합에 잘 맞는 직선 또는 초평면을 찾기 위한 방법
- 데이터를 그래프로 만들어 검토 
- 모델의 방정식
  - y' = b + (w1x1)
  - y'는 예측된 라벨(얻고자 하는 출력)
  - b는 편향(y절편), (w_0\)이라고도 함.
  - w1은 특성 1의 가중치 (기울기와 같은 개념)
  - x1은 특성(알려진 입력)
  - 예를 들어 새로운 분당 우는 횟수 x1에서 온도 y'를 추론하려면 x1 값을 이 모델에 삽입하면 됨
  - 세 가지 특성에 의존하는 모델의 방정식 : y' = b + (w1x1) + (w2x2) + (w3x3)  

### 1.3.2. 학습 및 손실

- 모델을 학습시킨다는 것 : 라벨이 있는 데이터에서 올바른 가중치와 편향값을 학습(결정)하는 것
- 경험적 위험 최소화 : 다양한 예를 검토하고 손실을 최소화 하는 모델을 찾아봄으로써 모델을 만드는 과정
- 손실 : 잘못된 예측에 대한 벌점. 즉 한 가지 예에서 모델의 예측이 얼마나 잘못되었는지 나타내는 수. 모델의 예측이 완벽하면 손실이 0.
- 모델 학습의 목표 : 모든 예에서 평균적으로 작은 손실을 갖는 가중치와 편향의 집합을 찾는 것

#### 1.3.2.1. 제곱 손실 (L2 손실)

- 잘 알려진 손실 함수
- 데이터 하나의 제곱 손실

```
 = the square of the difference between the label and the prediction
  = (observation - prediction(x))2
  = (y - y')2
```

- 평균 제곱 오차(MSE) : 예시당 평균 제곱 손실
- 식은 다음과 같음
  - MSE = \frac{1}{N} \sum_{(x,y)\in D} (y - prediction(x))^2 
  - (x, y)는 예
    - x는 특성 집합
    - y는 예의 라벨
  - prediction(x)는 특성집합 x와 결합된 가중치 및 편향의 함수
  - D는 (x,y) 쌍과 같이 여러 라벨이 있는 예가 포함된 데이터 세트
  - N은 D에 포함된 예의 수

## 1.4. 손실 줄이기

### 1.4.1. 반복 방식

- 머신러닝 모델은 반복을 통해 손실을 줄임
- 핫 앤 콜드 (숨겨진 물건을 찾는 놀이)와 비슷
  - '숨겨진 물건' : 최적 모델
  - 임의의 지점에서 시작(w1 = 0)해 시스템이 손실값을 알려줄때까지 기다림
  - 다른값을 추정 (w1 = 0.5)하고 시스템이 손실값을 알려줄때까지 기다림
  - 이런식으로 점점 가까워지게
- 머신러닝 모델은 가중치와 편향에 대한 초기 예상 값에서 시작해 예상 손실 값이 가장 적은 가중치와 편향을 학습할때까지 이러한 예상 값을 조정함

#### 1.4.1.1. 예시

- 이 모델은 하나 이상의 특성을 입력해 하나의 예측(y')을 출력
- y' = b + w1x1 
- 선형 회귀 문제에서 초기값은 크게 중요하지 않음
- b = 0, w1 = 0, x1 = 10이면
- y' = 0 + 0(10) = 0

##### 1.4.1.1.1. 손실 계산 과정 (제곱 손실 함수)

- 손실 함수는 두 개의 입력값을 취함
  - y' : 특성 x에 대한 모델의 예측값
  - y : 특성 x에 대한 올바른 라벨

##### 1.4.1.1.2. 매개변수 업데이트 계산 과정

- 손실 함수의 값을 검토해 b와 w1의 새로운 값을 생성

머신 러닝 시스템이 이러한 모든 특성을 모든 라벨과 대조하여 재평가하여 손실함수의 새로운 값 생성해 새로운 매개변수 출력한다고 가정

알고리즘이 손실값이 가장 낮은 모델 매개변수를 발견할때까지 반복 학습

이러한 과정을 전체 손실이 변하지 않거나 매우 느리게 변할때까지 계속 반복. 이때 모델이 **수렴**했다고 함. 

### 1.4.2. 경사하강법

- 볼록 문제에서 기울기가 정확하게 0인 지점인 최소값은 하나만 존재. 이 최소값에서 손실 함수가 수렴
- 전체 데이터 세트에 대해 상상할 수 있는 모든 w_1를 구하는건 비효율적
- 경사하강법
1. w_1에 대한 시작점 선택 (별로 중요하지는 않음)
2. 시작점에서 손실 곡선의 기울기(어느 방향이 더 정확한지, 부정확한지) 계산. 
3. 기울기는 항상 손실 함수 값이 가장 크게 증가하는 방향을 향하기 때문에 경사하강법은 기울기의 반대방향으로 이동.
4. 기울기의 크기의 일부를 시작점에 더함
5. 이 과정을 반복해서 최소값에 접근

### 학습률

- 경사하강법은 기울기에 학습률을 곱해 다음 지점 결정.
- 학습률이 너무 작으면 속도가 느림
- 학습률이 너무 크면 무질서하게 이탈
- 골디락스 학습률 : 최저점에 도달하는 단계수를 최소화 하는 학습률

### 확률적 경사하강법

- 지금까지 배치가 전체 데이터 세트라고 가정했는데 실제로는 수십 수천억개일 수 있음.
- 배치가 너무 크면 단일 반복으로 게산하는데 오랜 시간이 걸림
- 대량의 데이터 세트에는 중복 데이터 포함 가능성이 높아짐
- 확률적 경사하강법 : 반복당 하나의 예(배치크기 1)만 사용
- 미니 배치 확률적 경사하강법 : 절충안. 10개에서 1000개 사이의 예로 구성

## 1.5. TF 첫걸음

- 이 과정 대부분의 실습에서 tf.estimator 사용
- scikit-learn API와 호환


### 1.5.1. 프로그래밍 실습

- Pandas는 열 중심 데이터 분석 API
- Pandas의 기본 데이터 구조
  - DataFrame은 관계형 데이터 테이블 (행, 이름 지정된 열이 포함된) 
  - Series는 하나의 열. DataFrame에는 하나 이상의 Series와 각 Series의 이름이 포함

- 텐서플로우는 

# 2. 2주차

## 2.1. 검증

- 테스트 데이터만의 특성에 과적합한 모델이 나올수도 있음. 
- 그렇기에 모집단에 '검증 데이터'라는 3번째 데이터 세트를 만들어야 함.

### 2.1.1. 방식

- 학습 데이터로 학습시키고 데이터 평가에는 검증 데이터만 사용
- 검증 데이터에서 좋은 결과가 나올때까지 반복하면서 매개변수와 모델 수정
- 최종 테스트 데이터를 대상으로 모델 테스트
- 테스트 데이터에서 얻은 결과가 검증 데이터 결과와 일치하는지 확인
- 일치하지 않는다면 검증 세트에 과적합

### 2.1.2. 개선된 워크플로우

- 검증세트에서 가장 우수한 결과를 보이는 모델을 선택
- 테스트 세트를 기준으로 해당 모델을 재차 확인

### 프로그래밍 실습

## 2.2. 표현

- 표현 : 모델이 데이터의 핵심적인 특징을 들여다 볼 수 있는 관측 지점 제공해야...
- 모델을 학습시키려면 데이터를 가장 잘 표현하는 특성 세트 선택해야...
- 특성 추출 : 원시 데이터에서 특성을 추출하는 과정 (개발 시간의 75% 사용)
- 문자열 값이 있으면 원-핫 인코딩을 사용해 특성 벡터로 바꿈

- 좋은 특성이란? : 데이터 세트에서 0이 아닌 값으로 최소 몇 차례 이상 나타나고, 분명하고 명확한 의미를 가져야 함. 특이한 값을 가지면 안되고 시간이 지남에 따라 특성 값이 변하면 안됨. 또한 이상점 값을 가져서는 안됨.

### 특성 추출

- 비정제 데이터 -> 피쳐 벡터 (특성 벡터)
- 원시 데이터가 정수 및 부동소수점이면 특수한 인코딩은 필요하지 않음
- 그렇다면 문자열 값은? 특성 추출을 수행해 추출한 값을 숫자로 변환해야 함
  - 먼저 표현하려는 모든 특성의 문자열 값의 어휘를 정의
  - 이 어휘를 사용해 주어진 문자열 값을 바이너리 벡터로 표현하는 원 핫 인코딩을 만듬
  - 이 벡터의 길이는 어휘에 있는 요소의 수와 같음
- 범주형 값 매핑도 있음. 이 방식으로 인코딩 하면 둘 이상의 카테고리에 속하는 경우도 간단히 처리

### 좋은 특성의 조건

- 거의 사용되지 않는 불연속 특성 값 배제
- 가급적 분명하고 명확한 의미 부여
- '특수' 값을 실제 데이터와 혼용하지 말것
- 특성의 정의는 시간이 지나도 변하지 않아야 함.

### 데이터 정제

- 조정 : 부동소수점 특성값을 100~900등 자연 범위에서 0~1 또는 -1~+1등의 표준 범위로 변환하는 작업
- 여러 세트가 여러 특성으로 구성되어있을때 경사하강법이 더 빠르게 수렴 / NaN 트랩 방지 / 모델이 각 특성의 적절한 가중치를 익히는데 도움


#### 극단적 이상점 처리

- 극단점 이상점이 주는 영향을 최소화하려면 로그 취하기
- 그래도 이상하면 특성 값을 임의의 지점에서 잘라내어 제한을 두기 (예를 들면 4.0 이상의 값은 모두 4.0으로 인식하게)

#### 비닝

- 부동 소수점 특성 하나를 여러개의 개별 bool 특성으로 나누기
- 모델에서 각 위도에 대해 완전히 다른 가중치를 익힐 수 있음

#### 스크러빙

- 실무에서는 여러 가지 이유로 데이터 세트의 여러 예를 신뢰할 수 없음
  - 값 누락
  - 중복 예
  - 잘못된 라벨
  - 잘못된 특성 값
- 잘못된 예가 발견되면 일반적으로는 데이터 세트에서 삭제해서 해당 예를 수정

#### 철저한 데이터 파악

- 정상적인 데이터가 어떠한 모습이어야 하는지 항상 생각
- 데이터가 이러한 예상과 일치하는지 확인, 아니라면 그 이유 파악하기
- 학습 데이터가 대시보드 등의 다른 소스와 일치하는지 확인

### 프로그래밍 실습

## 2.3. 특성 교차

- 특성 교차 : 두 개 이상의 특성을 곱하여(교차하여) 구성되는 합성 특성
- 교차 곱을 이용하면 선형 모델 내에서 비선형성을 학습시킬 수 있음

### 비선형성 인코딩

- 비선형 문제를 해결하기 위해서 특성 교차를 만들어야 함
- 특성 교치 : 두 개 이상의 입력 특성을 곱하여 특성 공간에서 비선형성을 인코딩하는 합성 특성
- 확률적 경사하강법을 활용해 선형 모델을 효율적으로 학습시킬 수 있음


### 원-핫 벡터 교차

- 특성 하나만 사용하는 경우보다 훨씬 더 효과적으로 예측할 수 있음.

### 프로그래밍 실습

## 2.4. 정규화 : 단순성

- 과적합이라는 현상이 생김
- 과적합을 피하기 위해서는 정규화라는 방법을 사용
  - 조기 중단 (학습 데이터에 수렴하기전에 학습 멈춤)
  - 모델 복잡성에 페널티 주기 -> 구조적 위험 최소화

### L2 정규화

### 람다